{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "# HRV Pattern Recognition with CNN for Apple Watch Data\n",
    "This notebook implements a Convolutional Neural Network for detecting irregular heart patterns from Heart Rate Variability (HRV) time series data collected from Apple Watch continuous heart rate monitoring.\n",
    "\n",
    "## Key Features:\n",
    "- Uses HRV time series instead of ECG waveforms\n",
    "- Adapted CNN architecture for HRV pattern recognition\n",
    "- Detects irregular heart patterns from HRV variability\n",
    "- Optimized for Apple Watch continuous heart rate data\n",
    "- Multi-class classification for different HRV patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.stats import norm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Apple Watch HRV Data\n",
    "\n",
    "We'll create realistic HRV time series data that mimics Apple Watch continuous heart rate monitoring patterns. The data will include:\n",
    "- Normal HRV patterns (healthy variability)\n",
    "- Atrial Fibrillation patterns (irregular intervals)\n",
    "- Bradycardia patterns (slow heart rate with high variability)\n",
    "- Tachycardia patterns (fast heart rate with low variability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_hrv_data"
   },
   "outputs": [],
   "source": [
    "class HRVDataGenerator:\n",
    "    def __init__(self, sequence_length=200, sampling_rate=4):\n",
    "        \"\"\"\n",
    "        Initialize HRV data generator for Apple Watch-like data\n",
    "        \n",
    "        Args:\n",
    "            sequence_length: Number of RR intervals in each sequence\n",
    "            sampling_rate: Approximate sampling rate in Hz (Apple Watch ~4Hz)\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        \n",
    "    def generate_normal_hrv(self, base_hr=70):\n",
    "        \"\"\"\n",
    "        Generate normal HRV pattern with healthy variability\n",
    "        - Regular rhythm with natural variation\n",
    "        - RMSSD typically 20-50ms\n",
    "        - Heart rate around 60-100 bpm\n",
    "        \"\"\"\n",
    "        # Convert HR to RR intervals (in milliseconds)\n",
    "        base_rr = 60000 / base_hr  # Base RR interval in ms\n",
    "        \n",
    "        # Add respiratory sinus arrhythmia (natural variation)\n",
    "        respiratory_freq = 0.25  # 15 breaths per minute\n",
    "        time_points = np.linspace(0, self.sequence_length/self.sampling_rate, self.sequence_length)\n",
    "        respiratory_variation = 30 * np.sin(2 * np.pi * respiratory_freq * time_points)\n",
    "        \n",
    "        # Add random variation\n",
    "        random_variation = np.random.normal(0, 15, self.sequence_length)\n",
    "        \n",
    "        # Combine variations\n",
    "        rr_intervals = base_rr + respiratory_variation + random_variation\n",
    "        \n",
    "        # Ensure physiologically reasonable values (300-2000ms)\n",
    "        rr_intervals = np.clip(rr_intervals, 300, 2000)\n",
    "        \n",
    "        return rr_intervals\n",
    "    \n",
    "    def generate_afib_hrv(self, base_hr=85):\n",
    "        \"\"\"\n",
    "        Generate atrial fibrillation HRV pattern\n",
    "        - Highly irregular rhythm\n",
    "        - Large variations in RR intervals\n",
    "        - No clear pattern\n",
    "        \"\"\"\n",
    "        base_rr = 60000 / base_hr\n",
    "        \n",
    "        # Create highly irregular pattern\n",
    "        irregular_variation = np.random.normal(0, 80, self.sequence_length)\n",
    "        \n",
    "        # Add some clustering of short and long intervals\n",
    "        cluster_variation = np.zeros(self.sequence_length)\n",
    "        for i in range(0, self.sequence_length, 20):\n",
    "            cluster_size = min(np.random.randint(3, 8), self.sequence_length - i)\n",
    "            cluster_variation[i:i+cluster_size] = np.random.normal(0, 60, cluster_size)\n",
    "        \n",
    "        rr_intervals = base_rr + irregular_variation + cluster_variation\n",
    "        rr_intervals = np.clip(rr_intervals, 200, 2500)\n",
    "        \n",
    "        return rr_intervals\n",
    "    \n",
    "    def generate_bradycardia_hrv(self, base_hr=45):\n",
    "        \"\"\"\n",
    "        Generate bradycardia HRV pattern\n",
    "        - Slow heart rate (< 60 bpm)\n",
    "        - Higher variability due to increased parasympathetic activity\n",
    "        \"\"\"\n",
    "        base_rr = 60000 / base_hr\n",
    "        \n",
    "        # Enhanced respiratory variation\n",
    "        respiratory_freq = 0.2\n",
    "        time_points = np.linspace(0, self.sequence_length/self.sampling_rate, self.sequence_length)\n",
    "        respiratory_variation = 80 * np.sin(2 * np.pi * respiratory_freq * time_points)\n",
    "        \n",
    "        # Additional slow wave variation\n",
    "        slow_variation = 40 * np.sin(2 * np.pi * 0.1 * time_points)\n",
    "        \n",
    "        random_variation = np.random.normal(0, 25, self.sequence_length)\n",
    "        \n",
    "        rr_intervals = base_rr + respiratory_variation + slow_variation + random_variation\n",
    "        rr_intervals = np.clip(rr_intervals, 600, 2500)\n",
    "        \n",
    "        return rr_intervals\n",
    "    \n",
    "    def generate_tachycardia_hrv(self, base_hr=120):\n",
    "        \"\"\"\n",
    "        Generate tachycardia HRV pattern\n",
    "        - Fast heart rate (> 100 bpm)\n",
    "        - Reduced variability due to increased sympathetic activity\n",
    "        \"\"\"\n",
    "        base_rr = 60000 / base_hr\n",
    "        \n",
    "        # Reduced respiratory variation\n",
    "        respiratory_freq = 0.3\n",
    "        time_points = np.linspace(0, self.sequence_length/self.sampling_rate, self.sequence_length)\n",
    "        respiratory_variation = 10 * np.sin(2 * np.pi * respiratory_freq * time_points)\n",
    "        \n",
    "        # Small random variation\n",
    "        random_variation = np.random.normal(0, 8, self.sequence_length)\n",
    "        \n",
    "        rr_intervals = base_rr + respiratory_variation + random_variation\n",
    "        rr_intervals = np.clip(rr_intervals, 300, 1000)\n",
    "        \n",
    "        return rr_intervals\n",
    "    \n",
    "    def add_apple_watch_noise(self, rr_intervals):\n",
    "        \"\"\"\n",
    "        Add realistic Apple Watch measurement noise and artifacts\n",
    "        \"\"\"\n",
    "        # Add measurement noise (Â±2-5ms typical for optical sensors)\n",
    "        noise = np.random.normal(0, 3, len(rr_intervals))\n",
    "        \n",
    "        # Add occasional motion artifacts (sudden spikes)\n",
    "        artifact_probability = 0.02  # 2% chance of artifact per sample\n",
    "        artifacts = np.random.binomial(1, artifact_probability, len(rr_intervals))\n",
    "        artifact_magnitude = np.random.normal(0, 20, len(rr_intervals)) * artifacts\n",
    "        \n",
    "        return rr_intervals + noise + artifact_magnitude\n",
    "\n",
    "# Initialize the generator\n",
    "hrv_generator = HRVDataGenerator(sequence_length=200)\n",
    "\n",
    "# Generate synthetic dataset\n",
    "num_samples_per_class = 2500\n",
    "total_samples = num_samples_per_class * 4\n",
    "\n",
    "print(f\"Generating {total_samples} HRV sequences...\")\n",
    "\n",
    "# Create arrays to store data\n",
    "hrv_data = []\n",
    "labels = []\n",
    "label_names = ['Normal', 'Atrial Fibrillation', 'Bradycardia', 'Tachycardia']\n",
    "\n",
    "# Generate each class\n",
    "for class_idx, (class_name, generator_func) in enumerate([\n",
    "    ('Normal', hrv_generator.generate_normal_hrv),\n",
    "    ('Atrial Fibrillation', hrv_generator.generate_afib_hrv),\n",
    "    ('Bradycardia', hrv_generator.generate_bradycardia_hrv),\n",
    "    ('Tachycardia', hrv_generator.generate_tachycardia_hrv)\n",
    "]):\n",
    "    print(f\"Generating {class_name} patterns...\")\n",
    "    \n",
    "    for i in range(num_samples_per_class):\n",
    "        # Generate base pattern\n",
    "        if class_name == 'Normal':\n",
    "            base_hr = np.random.normal(75, 10)  # Vary base heart rate\n",
    "            base_hr = np.clip(base_hr, 60, 100)\n",
    "            rr_sequence = generator_func(base_hr)\n",
    "        elif class_name == 'Atrial Fibrillation':\n",
    "            base_hr = np.random.normal(90, 15)\n",
    "            base_hr = np.clip(base_hr, 70, 150)\n",
    "            rr_sequence = generator_func(base_hr)\n",
    "        elif class_name == 'Bradycardia':\n",
    "            base_hr = np.random.normal(50, 8)\n",
    "            base_hr = np.clip(base_hr, 35, 60)\n",
    "            rr_sequence = generator_func(base_hr)\n",
    "        else:  # Tachycardia\n",
    "            base_hr = np.random.normal(130, 20)\n",
    "            base_hr = np.clip(base_hr, 100, 180)\n",
    "            rr_sequence = generator_func(base_hr)\n",
    "        \n",
    "        # Add Apple Watch-like noise\n",
    "        rr_sequence = hrv_generator.add_apple_watch_noise(rr_sequence)\n",
    "        \n",
    "        hrv_data.append(rr_sequence)\n",
    "        labels.append(class_idx)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "hrv_data = np.array(hrv_data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Generated dataset shape: {hrv_data.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing and Feature Engineering for HRV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess_data"
   },
   "outputs": [],
   "source": [
    "def compute_hrv_features(rr_intervals):\n",
    "    \"\"\"\n",
    "    Compute time-domain and frequency-domain HRV features\n",
    "    \"\"\"\n",
    "    # Time-domain features\n",
    "    mean_rr = np.mean(rr_intervals)\n",
    "    std_rr = np.std(rr_intervals)\n",
    "    \n",
    "    # RMSSD (Root Mean Square of Successive Differences)\n",
    "    successive_diffs = np.diff(rr_intervals)\n",
    "    rmssd = np.sqrt(np.mean(successive_diffs**2))\n",
    "    \n",
    "    # pNN50 (percentage of successive RR intervals that differ by > 50ms)\n",
    "    nn50_count = np.sum(np.abs(successive_diffs) > 50)\n",
    "    pnn50 = (nn50_count / len(successive_diffs)) * 100\n",
    "    \n",
    "    return [mean_rr, std_rr, rmssd, pnn50]\n",
    "\n",
    "def preprocess_hrv_data(hrv_sequences):\n",
    "    \"\"\"\n",
    "    Preprocess HRV data for CNN input\n",
    "    \"\"\"\n",
    "    processed_sequences = []\n",
    "    \n",
    "    for sequence in hrv_sequences:\n",
    "        # Convert RR intervals to heart rate (BPM)\n",
    "        hr_sequence = 60000 / sequence  # Convert ms to BPM\n",
    "        \n",
    "        # Compute RR interval differences (captures variability)\n",
    "        rr_diffs = np.diff(sequence)\n",
    "        # Pad to maintain sequence length\n",
    "        rr_diffs = np.pad(rr_diffs, (0, 1), mode='edge')\n",
    "        \n",
    "        # Stack features: [RR intervals, HR, RR differences]\n",
    "        features = np.stack([sequence, hr_sequence, rr_diffs], axis=-1)\n",
    "        processed_sequences.append(features)\n",
    "    \n",
    "    return np.array(processed_sequences)\n",
    "\n",
    "# Preprocess the data\n",
    "print(\"Preprocessing HRV data...\")\n",
    "X_processed = preprocess_hrv_data(hrv_data)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "original_shape = X_processed.shape\n",
    "X_normalized = scaler.fit_transform(X_processed.reshape(-1, X_processed.shape[-1]))\n",
    "X_normalized = X_normalized.reshape(original_shape)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_categorical = tf.keras.utils.to_categorical(labels, num_classes=4)\n",
    "\n",
    "print(f\"Processed data shape: {X_normalized.shape}\")\n",
    "print(f\"Categorical labels shape: {y_categorical.shape}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y_categorical, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize HRV Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_data"
   },
   "outputs": [],
   "source": [
    "# Visualize sample patterns from each class\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('HRV Patterns for Different Heart Conditions', fontsize=16)\n",
    "\n",
    "for class_idx in range(4):\n",
    "    # Find first sample of this class\n",
    "    sample_idx = np.where(labels == class_idx)[0][0]\n",
    "    sample_data = hrv_data[sample_idx]\n",
    "    \n",
    "    # Plot RR intervals\n",
    "    axes[0, class_idx].plot(sample_data, 'b-', linewidth=1)\n",
    "    axes[0, class_idx].set_title(f'{label_names[class_idx]}\\nRR Intervals')\n",
    "    axes[0, class_idx].set_ylabel('RR Interval (ms)')\n",
    "    axes[0, class_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot heart rate\n",
    "    hr_data = 60000 / sample_data\n",
    "    axes[1, class_idx].plot(hr_data, 'r-', linewidth=1)\n",
    "    axes[1, class_idx].set_title('Heart Rate')\n",
    "    axes[1, class_idx].set_ylabel('Heart Rate (BPM)')\n",
    "    axes[1, class_idx].set_xlabel('Time (samples)')\n",
    "    axes[1, class_idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show HRV statistics for each class\n",
    "print(\"\\nHRV Statistics by Class:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for class_idx in range(4):\n",
    "    class_samples = hrv_data[labels == class_idx]\n",
    "    \n",
    "    # Compute statistics\n",
    "    mean_rr = np.mean([np.mean(sample) for sample in class_samples])\n",
    "    mean_hr = 60000 / mean_rr\n",
    "    mean_rmssd = np.mean([np.sqrt(np.mean(np.diff(sample)**2)) for sample in class_samples])\n",
    "    \n",
    "    print(f\"\\n{label_names[class_idx]}:\")\n",
    "    print(f\"  Mean RR Interval: {mean_rr:.1f} ms\")\n",
    "    print(f\"  Mean Heart Rate: {mean_hr:.1f} BPM\")\n",
    "    print(f\"  Mean RMSSD: {mean_rmssd:.1f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Advanced CNN Architecture for HRV Pattern Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "build_model"
   },
   "outputs": [],
   "source": [
    "def build_hrv_cnn_model(input_shape, num_classes=4):\n",
    "    \"\"\"\n",
    "    Build advanced CNN architecture optimized for HRV pattern recognition\n",
    "    \n",
    "    Architecture features:\n",
    "    - Multi-scale convolutions to capture different temporal patterns\n",
    "    - Residual connections for better gradient flow\n",
    "    - Attention mechanism to focus on important time segments\n",
    "    - Dropout and batch normalization for regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Multi-scale feature extraction\n",
    "    # Short-term patterns (kernel size 3-5)\n",
    "    conv1_short = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1_short = tf.keras.layers.BatchNormalization()(conv1_short)\n",
    "    \n",
    "    conv2_short = tf.keras.layers.Conv1D(32, 5, activation='relu', padding='same')(inputs)\n",
    "    conv2_short = tf.keras.layers.BatchNormalization()(conv2_short)\n",
    "    \n",
    "    # Medium-term patterns (kernel size 7-11)\n",
    "    conv1_med = tf.keras.layers.Conv1D(32, 7, activation='relu', padding='same')(inputs)\n",
    "    conv1_med = tf.keras.layers.BatchNormalization()(conv1_med)\n",
    "    \n",
    "    conv2_med = tf.keras.layers.Conv1D(32, 11, activation='relu', padding='same')(inputs)\n",
    "    conv2_med = tf.keras.layers.BatchNormalization()(conv2_med)\n",
    "    \n",
    "    # Concatenate multi-scale features\n",
    "    multi_scale = tf.keras.layers.Concatenate()([conv1_short, conv2_short, conv1_med, conv2_med])\n",
    "    \n",
    "    # First residual block\n",
    "    res1 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(multi_scale)\n",
    "    res1 = tf.keras.layers.BatchNormalization()(res1)\n",
    "    res1 = tf.keras.layers.Conv1D(64, 3, padding='same')(res1)\n",
    "    res1 = tf.keras.layers.BatchNormalization()(res1)\n",
    "    \n",
    "    # Adjust dimensions for residual connection\n",
    "    multi_scale_proj = tf.keras.layers.Conv1D(64, 1, padding='same')(multi_scale)\n",
    "    res1_output = tf.keras.layers.Add()([res1, multi_scale_proj])\n",
    "    res1_output = tf.keras.layers.Activation('relu')(res1_output)\n",
    "    res1_output = tf.keras.layers.MaxPooling1D(2)(res1_output)\n",
    "    res1_output = tf.keras.layers.Dropout(0.2)(res1_output)\n",
    "    \n",
    "    # Second residual block\n",
    "    res2 = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(res1_output)\n",
    "    res2 = tf.keras.layers.BatchNormalization()(res2)\n",
    "    res2 = tf.keras.layers.Conv1D(128, 3, padding='same')(res2)\n",
    "    res2 = tf.keras.layers.BatchNormalization()(res2)\n",
    "    \n",
    "    res1_proj = tf.keras.layers.Conv1D(128, 1, padding='same')(res1_output)\n",
    "    res2_output = tf.keras.layers.Add()([res2, res1_proj])\n",
    "    res2_output = tf.keras.layers.Activation('relu')(res2_output)\n",
    "    res2_output = tf.keras.layers.MaxPooling1D(2)(res2_output)\n",
    "    res2_output = tf.keras.layers.Dropout(0.3)(res2_output)\n",
    "    \n",
    "    # Attention mechanism\n",
    "    attention_weights = tf.keras.layers.Conv1D(1, 1, activation='softmax', padding='same')(res2_output)\n",
    "    attended_features = tf.keras.layers.Multiply()([res2_output, attention_weights])\n",
    "    \n",
    "    # Global average pooling and max pooling\n",
    "    gap = tf.keras.layers.GlobalAveragePooling1D()(attended_features)\n",
    "    gmp = tf.keras.layers.GlobalMaxPooling1D()(attended_features)\n",
    "    \n",
    "    # Combine pooled features\n",
    "    combined = tf.keras.layers.Concatenate()([gap, gmp])\n",
    "    \n",
    "    # Dense layers with dropout\n",
    "    dense1 = tf.keras.layers.Dense(256, activation='relu')(combined)\n",
    "    dense1 = tf.keras.layers.BatchNormalization()(dense1)\n",
    "    dense1 = tf.keras.layers.Dropout(0.4)(dense1)\n",
    "    \n",
    "    dense2 = tf.keras.layers.Dense(128, activation='relu')(dense1)\n",
    "    dense2 = tf.keras.layers.BatchNormalization()(dense2)\n",
    "    dense2 = tf.keras.layers.Dropout(0.3)(dense2)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(dense2)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "input_shape = X_train.shape[1:]\n",
    "model = build_hrv_cnn_model(input_shape, num_classes=4)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()\n",
    "\n",
    "# Plot model architecture\n",
    "tf.keras.utils.plot_model(\n",
    "    model, \n",
    "    to_file='hrv_cnn_architecture.png', \n",
    "    show_shapes=True, \n",
    "    show_layer_names=True,\n",
    "    rankdir='TB'\n",
    ")\n",
    "\n",
    "print(f\"\\nModel built successfully!\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the HRV CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=8,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_hrv_cnn_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "print(\"This may take several minutes depending on your hardware.\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_model"
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model('best_hrv_cnn_model.h5')\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_loss, test_accuracy, test_precision, test_recall = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Loss: {test_loss:.4f}\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1-Score: {2 * (test_precision * test_recall) / (test_precision + test_recall):.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test_classes, y_pred_classes, target_names=label_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_names, yticklabels=label_names)\n",
    "plt.title('Confusion Matrix - HRV Pattern Classification')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Training History', fontsize=16)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[0, 0].set_title('Model Accuracy')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0, 1].set_title('Model Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].plot(history.history['precision'], label='Training Precision')\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Validation Precision')\n",
    "axes[1, 0].set_title('Model Precision')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].plot(history.history['recall'], label='Training Recall')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Validation Recall')\n",
    "axes[1, 1].set_title('Model Recall')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Apple Watch Integration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apple_watch_integration"
   },
   "outputs": [],
   "source": [
    "class AppleWatchHRVAnalyzer:\n",
    "    \"\"\"\n",
    "    Class for analyzing Apple Watch HRV data in real-time\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path='best_hrv_cnn_model.h5'):\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.scaler = scaler  # Use the fitted scaler from training\n",
    "        self.label_names = ['Normal', 'Atrial Fibrillation', 'Bradycardia', 'Tachycardia']\n",
    "        self.sequence_length = 200\n",
    "        \n",
    "    def preprocess_apple_watch_data(self, heart_rate_data, timestamps):\n",
    "        \"\"\"\n",
    "        Convert Apple Watch heart rate data to RR intervals\n",
    "        \n",
    "        Args:\n",
    "            heart_rate_data: List of heart rate values in BPM\n",
    "            timestamps: List of timestamps for each measurement\n",
    "        \n",
    "        Returns:\n",
    "            RR intervals in milliseconds\n",
    "        \"\"\"\n",
    "        # Convert HR to RR intervals\n",
    "        rr_intervals = 60000 / np.array(heart_rate_data)  # Convert to ms\n",
    "        \n",
    "        # Apply smoothing to reduce noise\n",
    "        if len(rr_intervals) > 5:\n",
    "            rr_intervals = signal.savgol_filter(rr_intervals, 5, 3)\n",
    "        \n",
    "        return rr_intervals\n",
    "    \n",
    "    def create_sliding_windows(self, rr_intervals, window_size=200, overlap=0.5):\n",
    "        \"\"\"\n",
    "        Create sliding windows from continuous RR interval data\n",
    "        \"\"\"\n",
    "        step_size = int(window_size * (1 - overlap))\n",
    "        windows = []\n",
    "        \n",
    "        for i in range(0, len(rr_intervals) - window_size + 1, step_size):\n",
    "            window = rr_intervals[i:i + window_size]\n",
    "            windows.append(window)\n",
    "        \n",
    "        return np.array(windows)\n",
    "    \n",
    "    def analyze_hrv_pattern(self, rr_intervals):\n",
    "        \"\"\"\n",
    "        Analyze HRV pattern and return prediction with confidence\n",
    "        \"\"\"\n",
    "        if len(rr_intervals) < self.sequence_length:\n",
    "            # Pad with mean if sequence is too short\n",
    "            mean_rr = np.mean(rr_intervals)\n",
    "            padded = np.pad(rr_intervals, (0, self.sequence_length - len(rr_intervals)), \n",
    "                          mode='constant', constant_values=mean_rr)\n",
    "            rr_intervals = padded\n",
    "        elif len(rr_intervals) > self.sequence_length:\n",
    "            # Take the most recent data\n",
    "            rr_intervals = rr_intervals[-self.sequence_length:]\n",
    "        \n",
    "        # Preprocess similar to training data\n",
    "        hr_sequence = 60000 / rr_intervals\n",
    "        rr_diffs = np.diff(rr_intervals)\n",
    "        rr_diffs = np.pad(rr_diffs, (0, 1), mode='edge')\n",
    "        \n",
    "        # Stack features\n",
    "        features = np.stack([rr_intervals, hr_sequence, rr_diffs], axis=-1)\n",
    "        features = features.reshape(1, *features.shape)\n",
    "        \n",
    "        # Normalize\n",
    "        original_shape = features.shape\n",
    "        features_normalized = self.scaler.transform(features.reshape(-1, features.shape[-1]))\n",
    "        features_normalized = features_normalized.reshape(original_shape)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = self.model.predict(features_normalized, verbose=0)\n",
    "        \n",
    "        # Get predicted class and confidence\n",
    "        predicted_class = np.argmax(prediction[0])\n",
    "        confidence = np.max(prediction[0])\n",
    "        \n",
    "        return {\n",
    "            'predicted_condition': self.label_names[predicted_class],\n",
    "            'confidence': confidence,\n",
    "            'probabilities': dict(zip(self.label_names, prediction[0])),\n",
    "            'risk_level': self.get_risk_level(predicted_class, confidence)\n",
    "        }\n",
    "    \n",
    "    def get_risk_level(self, predicted_class, confidence):\n",
    "        \"\"\"\n",
    "        Determine risk level based on prediction and confidence\n",
    "        \"\"\"\n",
    "        if predicted_class == 0:  # Normal\n",
    "            return 'Low' if confidence > 0.8 else 'Moderate'\n",
    "        elif predicted_class == 1:  # Atrial Fibrillation\n",
    "            return 'High' if confidence > 0.7 else 'Moderate'\n",
    "        elif predicted_class == 2:  # Bradycardia\n",
    "            return 'Moderate' if confidence > 0.6 else 'Low'\n",
    "        else:  # Tachycardia\n",
    "            return 'Moderate' if confidence > 0.6 else 'Low'\n",
    "    \n",
    "    def generate_health_report(self, rr_intervals, analysis_result):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive health report\n",
    "        \"\"\"\n",
    "        # Calculate HRV metrics\n",
    "        mean_rr = np.mean(rr_intervals)\n",
    "        mean_hr = 60000 / mean_rr\n",
    "        rmssd = np.sqrt(np.mean(np.diff(rr_intervals)**2))\n",
    "        pnn50 = (np.sum(np.abs(np.diff(rr_intervals)) > 50) / len(np.diff(rr_intervals))) * 100\n",
    "        \n",
    "        report = {\n",
    "            'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'hrv_metrics': {\n",
    "                'mean_heart_rate_bpm': round(mean_hr, 1),\n",
    "                'mean_rr_interval_ms': round(mean_rr, 1),\n",
    "                'rmssd_ms': round(rmssd, 1),\n",
    "                'pnn50_percent': round(pnn50, 1)\n",
    "            },\n",
    "            'prediction': analysis_result,\n",
    "            'recommendations': self.get_recommendations(analysis_result)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def get_recommendations(self, analysis_result):\n",
    "        \"\"\"\n",
    "        Provide health recommendations based on analysis\n",
    "        \"\"\"\n",
    "        condition = analysis_result['predicted_condition']\n",
    "        risk_level = analysis_result['risk_level']\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        if condition == 'Normal':\n",
    "            recommendations = [\n",
    "                \"Your heart rhythm appears normal. Continue maintaining a healthy lifestyle.\",\n",
    "                \"Regular exercise and stress management support good heart health.\",\n",
    "                \"Continue monitoring your heart rate variability.\"\n",
    "            ]\n",
    "        elif condition == 'Atrial Fibrillation':\n",
    "            recommendations = [\n",
    "                \"â ï¸ Irregular heart rhythm detected. Consider consulting a healthcare provider.\",\n",
    "                \"Avoid excessive caffeine and alcohol consumption.\",\n",
    "                \"Monitor symptoms like palpitations, shortness of breath, or chest discomfort.\",\n",
    "                \"Keep a record of when irregular rhythms occur.\"\n",
    "            ]\n",
    "        elif condition == 'Bradycardia':\n",
    "            recommendations = [\n",
    "                \"Slow heart rate detected. Monitor for symptoms like fatigue or dizziness.\",\n",
    "                \"If you're an athlete, this may be normal. Otherwise, consider medical evaluation.\",\n",
    "                \"Stay hydrated and avoid sudden position changes.\"\n",
    "            ]\n",
    "        elif condition == 'Tachycardia':\n",
    "            recommendations = [\n",
    "                \"Elevated heart rate detected. Consider factors like stress, caffeine, or physical activity.\",\n",
    "                \"Practice relaxation techniques and ensure adequate rest.\",\n",
    "                \"If persistent without clear cause, consult a healthcare provider.\"\n",
    "            ]\n",
    "        \n",
    "        if risk_level == 'High':\n",
    "            recommendations.insert(0, \"ð¨ HIGH RISK: Seek immediate medical attention if experiencing symptoms.\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Initialize the analyzer\n",
    "hrv_analyzer = AppleWatchHRVAnalyzer()\n",
    "\n",
    "print(\"Apple Watch HRV Analyzer initialized successfully!\")\n",
    "print(\"Ready to analyze heart rate variability patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Demonstration with Simulated Apple Watch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_analysis"
   },
   "outputs": [],
   "source": [
    "# Simulate Apple Watch heart rate data for different scenarios\n",
    "def simulate_apple_watch_data(condition='normal', duration_minutes=5):\n",
    "    \"\"\"\n",
    "    Simulate Apple Watch heart rate data\n",
    "    \"\"\"\n",
    "    # Apple Watch typically samples every 15-60 seconds during passive monitoring\n",
    "    # During workout mode, it can sample every 1-5 seconds\n",
    "    sampling_interval = 15  # seconds\n",
    "    num_samples = duration_minutes * 60 // sampling_interval\n",
    "    \n",
    "    timestamps = pd.date_range(\n",
    "        start=pd.Timestamp.now() - pd.Timedelta(minutes=duration_minutes),\n",
    "        periods=num_samples,\n",
    "        freq=f'{sampling_interval}S'\n",
    "    )\n",
    "    \n",
    "    if condition == 'normal':\n",
    "        base_hr = 72\n",
    "        hr_data = base_hr + np.random.normal(0, 8, num_samples)\n",
    "        # Add some natural variation\n",
    "        hr_data += 5 * np.sin(np.linspace(0, 4*np.pi, num_samples))  # Respiratory variation\n",
    "        \n",
    "    elif condition == 'afib':\n",
    "        base_hr = 85\n",
    "        hr_data = base_hr + np.random.normal(0, 20, num_samples)\n",
    "        # Add irregular spikes\n",
    "        for i in range(0, num_samples, 5):\n",
    "            if np.random.random() < 0.3:  # 30% chance of irregular beat\n",
    "                hr_data[i:i+2] += np.random.normal(15, 10, min(2, num_samples-i))\n",
    "                \n",
    "    elif condition == 'bradycardia':\n",
    "        base_hr = 48\n",
    "        hr_data = base_hr + np.random.normal(0, 6, num_samples)\n",
    "        # Higher variability\n",
    "        hr_data += 8 * np.sin(np.linspace(0, 2*np.pi, num_samples))\n",
    "        \n",
    "    elif condition == 'tachycardia':\n",
    "        base_hr = 130\n",
    "        hr_data = base_hr + np.random.normal(0, 5, num_samples)\n",
    "        # Less variability\n",
    "        hr_data += 2 * np.sin(np.linspace(0, np.pi, num_samples))\n",
    "    \n",
    "    # Ensure physiologically reasonable values\n",
    "    hr_data = np.clip(hr_data, 30, 220)\n",
    "    \n",
    "    return hr_data, timestamps\n",
    "\n",
    "# Demonstrate analysis for different conditions\n",
    "conditions = ['normal', 'afib', 'bradycardia', 'tachycardia']\n",
    "condition_names = ['Normal', 'Atrial Fibrillation', 'Bradycardia', 'Tachycardia']\n",
    "\n",
    "print(\"ð Apple Watch HRV Analysis Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, condition in enumerate(conditions):\n",
    "    print(f\"\\nð± Simulating {condition_names[i]} condition...\")\n",
    "    \n",
    "    # Generate simulated data\n",
    "    hr_data, timestamps = simulate_apple_watch_data(condition, duration_minutes=10)\n",
    "    \n",
    "    # Convert to RR intervals\n",
    "    rr_intervals = hrv_analyzer.preprocess_apple_watch_data(hr_data, timestamps)\n",
    "    \n",
    "    # Analyze the pattern\n",
    "    analysis_result = hrv_analyzer.analyze_hrv_pattern(rr_intervals)\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    health_report = hrv_analyzer.generate_health_report(rr_intervals, analysis_result)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nð Analysis Results for {condition_names[i]}:\")\n",
    "    print(f\"Predicted Condition: {analysis_result['predicted_condition']}\")\n",
    "    print(f\"Confidence: {analysis_result['confidence']:.1%}\")\n",
    "    print(f\"Risk Level: {analysis_result['risk_level']}\")\n",
    "    \n",
    "    print(f\"\\nð HRV Metrics:\")\n",
    "    metrics = health_report['hrv_metrics']\n",
    "    print(f\"Mean Heart Rate: {metrics['mean_heart_rate_bpm']} BPM\")\n",
    "    print(f\"RMSSD: {metrics['rmssd_ms']} ms\")\n",
    "    print(f\"pNN50: {metrics['pnn50_percent']}%\")\n",
    "    \n",
    "    print(f\"\\nð¡ Recommendations:\")\n",
    "    for rec in health_report['recommendations']:\n",
    "        print(f\"  â¢ {rec}\")\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Real-time Monitoring Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "realtime_monitoring"
   },
   "outputs": [],
   "source": [
    "# Simulate real-time monitoring\n",
    "def simulate_realtime_monitoring(duration_minutes=30, condition_changes=None):\n",
    "    \"\"\"\n",
    "    Simulate real-time HRV monitoring with condition changes\n",
    "    \n",
    "    Args:\n",
    "        duration_minutes: Total monitoring duration\n",
    "        condition_changes: List of (time_minute, new_condition) tuples\n",
    "    \"\"\"\n",
    "    if condition_changes is None:\n",
    "        condition_changes = [\n",
    "            (0, 'normal'),\n",
    "            (10, 'tachycardia'),  # Stress/exercise starts\n",
    "            (20, 'afib'),         # Irregular rhythm develops\n",
    "            (25, 'normal')        # Returns to normal\n",
    "        ]\n",
    "    \n",
    "    print(\"ð Real-time HRV Monitoring Simulation\")\n",
    "    print(f\"Duration: {duration_minutes} minutes\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_hr_data = []\n",
    "    all_timestamps = []\n",
    "    condition_log = []\n",
    "    \n",
    "    # Generate data for each condition period\n",
    "    for i, (start_time, condition) in enumerate(condition_changes):\n",
    "        # Determine duration for this condition\n",
    "        if i < len(condition_changes) - 1:\n",
    "            end_time = condition_changes[i + 1][0]\n",
    "        else:\n",
    "            end_time = duration_minutes\n",
    "        \n",
    "        period_duration = end_time - start_time\n",
    "        \n",
    "        if period_duration > 0:\n",
    "            hr_data, timestamps = simulate_apple_watch_data(condition, period_duration)\n",
    "            \n",
    "            # Adjust timestamps to be relative to monitoring start\n",
    "            adjusted_timestamps = timestamps + pd.Timedelta(minutes=start_time)\n",
    "            \n",
    "            all_hr_data.extend(hr_data)\n",
    "            all_timestamps.extend(adjusted_timestamps)\n",
    "            condition_log.append((start_time, condition, len(hr_data)))\n",
    "    \n",
    "    # Analyze the complete data in sliding windows\n",
    "    all_hr_data = np.array(all_hr_data)\n",
    "    rr_intervals = hrv_analyzer.preprocess_apple_watch_data(all_hr_data, all_timestamps)\n",
    "    \n",
    "    # Create sliding windows for analysis\n",
    "    windows = hrv_analyzer.create_sliding_windows(rr_intervals, window_size=200, overlap=0.7)\n",
    "    \n",
    "    print(f\"\\nð Analyzing {len(windows)} time windows...\")\n",
    "    \n",
    "    # Analyze each window\n",
    "    results = []\n",
    "    for i, window in enumerate(windows):\n",
    "        analysis = hrv_analyzer.analyze_hrv_pattern(window)\n",
    "        \n",
    "        window_time = i * (200 * 0.3) / 4  # Approximate time in minutes\n",
    "        results.append({\n",
    "            'time_minutes': round(window_time, 1),\n",
    "            'condition': analysis['predicted_condition'],\n",
    "            'confidence': analysis['confidence'],\n",
    "            'risk_level': analysis['risk_level']\n",
    "        })\n",
    "    \n",
    "    # Display timeline\n",
    "    print(\"\\nâ° Timeline Analysis:\")\n",
    "    print(f\"{'Time (min)':<10} {'Detected Condition':<18} {'Confidence':<12} {'Risk Level':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for result in results[::3]:  # Show every 3rd result to avoid clutter\n",
    "        print(f\"{result['time_minutes']:<10} {result['condition']:<18} {result['confidence']:<12.1%} {result['risk_level']:<10}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    condition_counts = {}\n",
    "    for result in results:\n",
    "        condition = result['condition']\n",
    "        condition_counts[condition] = condition_counts.get(condition, 0) + 1\n",
    "    \n",
    "    print(f\"\\nð Summary:\")\n",
    "    print(f\"Total analysis windows: {len(results)}\")\n",
    "    print(f\"Condition distribution:\")\n",
    "    for condition, count in condition_counts.items():\n",
    "        percentage = (count / len(results)) * 100\n",
    "        print(f\"  {condition}: {count} windows ({percentage:.1f}%)\")\n",
    "    \n",
    "    # High-risk periods\n",
    "    high_risk_periods = [r for r in results if r['risk_level'] == 'High']\n",
    "    if high_risk_periods:\n",
    "        print(f\"\\nâ ï¸ High-risk periods detected: {len(high_risk_periods)}\")\n",
    "        for period in high_risk_periods[:5]:  # Show first 5\n",
    "            print(f\"  Time {period['time_minutes']} min: {period['condition']} ({period['confidence']:.1%})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run real-time monitoring simulation\n",
    "monitoring_results = simulate_realtime_monitoring(duration_minutes=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Model Saving and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# Save the complete model and preprocessing components\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "print(\"ð¾ Saving model and components...\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = 'apple_watch_hrv_cnn_model.h5'\n",
    "best_model.save(model_save_path)\n",
    "print(f\"â Model saved to: {model_save_path}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_save_path = 'hrv_scaler.pickle'\n",
    "with open(scaler_save_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"â Scaler saved to: {scaler_save_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_version': '1.0',\n",
    "    'created_date': pd.Timestamp.now().isoformat(),\n",
    "    'model_type': 'CNN for HRV Pattern Recognition',\n",
    "    'input_shape': input_shape,\n",
    "    'num_classes': 4,\n",
    "    'class_names': label_names,\n",
    "    'sequence_length': 200,\n",
    "    'features': ['RR intervals (ms)', 'Heart Rate (BPM)', 'RR differences (ms)'],\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'test_precision': float(test_precision),\n",
    "    'test_recall': float(test_recall),\n",
    "    'total_parameters': int(best_model.count_params()),\n",
    "    'training_samples': len(X_train),\n",
    "    'validation_samples': len(X_val),\n",
    "    'test_samples': len(X_test)\n",
    "}\n",
    "\n",
    "metadata_save_path = 'model_metadata.json'\n",
    "with open(metadata_save_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "print(f\"â Metadata saved to: {metadata_save_path}\")\n",
    "\n",
    "# Create a deployment package info\n",
    "deployment_info = \"\"\"\n",
    "ð Apple Watch HRV CNN Model - Deployment Package\n",
    "\n",
    "Files included:\n",
    "1. apple_watch_hrv_cnn_model.h5 - Trained CNN model\n",
    "2. hrv_scaler.pickle - Feature scaler for preprocessing\n",
    "3. model_metadata.json - Model information and performance metrics\n",
    "4. HRV_CNN_Analysis.ipynb - Complete training notebook\n",
    "\n",
    "Usage:\n",
    "1. Load the model: tf.keras.models.load_model('apple_watch_hrv_cnn_model.h5')\n",
    "2. Load the scaler: pickle.load(open('hrv_scaler.pickle', 'rb'))\n",
    "3. Use AppleWatchHRVAnalyzer class for inference\n",
    "\n",
    "Model Performance:\n",
    "- Test Accuracy: {:.2%}\n",
    "- Test Precision: {:.2%}\n",
    "- Test Recall: {:.2%}\n",
    "- Total Parameters: {:,}\n",
    "\n",
    "Supported Conditions:\n",
    "- Normal heart rhythm\n",
    "- Atrial Fibrillation\n",
    "- Bradycardia\n",
    "- Tachycardia\n",
    "\n",
    "Input Requirements:\n",
    "- Heart rate data from Apple Watch (continuous monitoring)\n",
    "- Minimum 200 RR intervals for reliable analysis\n",
    "- Sampling rate: ~4 Hz (typical for Apple Watch)\n",
    "\n",
    "Integration Notes:\n",
    "- Compatible with HealthKit for Apple Watch data\n",
    "- Real-time analysis with sliding window approach\n",
    "- Risk level assessment and health recommendations\n",
    "- Suitable for continuous monitoring applications\n",
    "\"\"\".format(\n",
    "    test_accuracy, test_precision, test_recall, best_model.count_params()\n",
    ")\n",
    "\n",
    "with open('DEPLOYMENT_INFO.txt', 'w') as f:\n",
    "    f.write(deployment_info)\n",
    "\n",
    "print(f\"â Deployment info saved to: DEPLOYMENT_INFO.txt\")\n",
    "\n",
    "print(\"\\nð Model training and saving completed successfully!\")\n",
    "print(\"\\nð± Your Apple Watch HRV analysis model is ready for deployment.\")\n",
    "print(f\"\\nð Final Model Performance:\")\n",
    "print(f\"   Accuracy: {test_accuracy:.2%}\")\n",
    "print(f\"   Precision: {test_precision:.2%}\")\n",
    "print(f\"   Recall: {test_recall:.2%}\")\n",
    "print(f\"   Parameters: {best_model.count_params():,}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}